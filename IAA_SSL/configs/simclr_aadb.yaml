name: simclr-aadb-experiment-resnet50-pretrain_epoch100
amp: True
num_train_iter: 34
data_dir: '/home/xiexie/data/AADB'
log_dir: '/home/xiexie/self-supervesion/SimSiam22/result'
ckpt_dir: '/home/xiexie/self-supervesion/SimSiam22/result'

dataset: 
  name: AADB
  image_size: 224
  num_workers: 16

model: 
  name: simclr
  backbone: resnet50
  proj_layers: 2

pretrain:
  optimizer:
    name: lars_simclr
    weight_decay: 1.5e-6
    momentum: 0.9
  warmup_epochs: 10
  warmup_lr: 0
  base_lr: 0.3
  final_lr: 0
  num_epochs: 100 # this parameter influence the lr decay
  stop_at_epoch: 100 # has to be smaller than num_epochs
  batch_size: 64
  save_interval: 20
  milestones: [99]
  knn_monitor: False # knn monitor will take more time
  knn_interval: 1
  knn_k: 200
eval: # linear evaluation, False will turn off automatic evaluation after training
  optimizer:
    name: SGD
    weight_decay: 0
    momentum: 0.9
  task_id: simclr_eval_1.0data_t
  num_attr: 12
  data_name: train
  warmup_lr: 0
  warmup_epochs: 0
  base_lr: 0.001
  final_lr: 0
  batch_size: 128
  num_epochs: 100

fine_tune:
  optimizer:
    name: adam
  task_id: simclr_ft_1.0data
  data_name: train
  dropout: 0.0
  num_classes: 12
  epochs: 32
  batch_size: 128
  base_lr: 0.0001
  seed: 42

distillation:
  optimizer:
    name: adam
  task_id: simclr_dl_1.0data
  dropout: 0
  num_classes: 12
  epochs: 32
  batch_size: 96
  base_lr: 0.0001
  seed: 42
  alpha: 0.6
  T: 8.0
  kd_mode: mse

logger:
  tensorboard: True
  matplotlib: True

seed: 0 # None type for yaml file
# two things might lead to stochastic behavior other than seed:
# worker_init_fn from dataloader and torch.nn.functional.interpolate 
# (keep this in mind if you want to achieve 100% deterministic)




